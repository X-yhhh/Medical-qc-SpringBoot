import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import pandas as pd
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

# ======================
# é…ç½®
# ======================
DATA_DIR = "data/head_ct"
LABELS_FILE = "data/labels.csv"
MODEL_SAVE_PATH = "models/hemorrhage_model_best.pth"
BATCH_SIZE = 8
EPOCHS = 150  # å¢åŠ æœ€å¤§è½®æ¬¡
LEARNING_RATE = 0.0005  # ç¨å¾®é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç²¾ç»†
IMAGE_SIZE = (224, 224)
PATIENCE = 20  # æ—©åœè€å¿ƒå€¼å¢åŠ 
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SEED = 42  # å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°

# è®¾ç½®éšæœºç§å­
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
os.makedirs("models", exist_ok=True)
os.makedirs("results", exist_ok=True)  # ç”¨äºä¿å­˜ç»“æœå›¾è¡¨


# ======================
# æ•°æ®é›†ç±»
# ======================
class HemorrhageDataset(Dataset):
    def __init__(self, image_ids, labels, transform=None):
        self.image_ids = image_ids
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        label = self.labels[idx]
        # å›¾åƒè·¯å¾„: data/head_ct/001.png
        img_path = os.path.join(DATA_DIR, f"{img_id:03d}.png")
        image = Image.open(img_path).convert("L")  # ç°åº¦å›¾
        if self.transform:
            image = self.transform(image)
        return image, torch.tensor(label, dtype=torch.long)


# ======================
# æ¨¡å‹å®šä¹‰ï¼ˆæ›´å¼ºå¤§ã€æ›´å¥å£®ï¼‰
# ======================
class Classifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # Input: (224, 224)
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (112, 112)
            nn.Dropout2d(0.1),  # è½»å¾®Dropout

            # Block 2
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (56, 56)
            nn.Dropout2d(0.1),

            # Block 3
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (28, 28)
            nn.Dropout2d(0.1),

            # Block 4
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (14, 14)
            nn.Dropout2d(0.1),

            # Global Average Pooling
            nn.AdaptiveAvgPool2d((1, 1)),  # Output: (1, 1)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.5),  # Classifierä¸­çš„Dropout
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(64, 2)  # äºŒåˆ†ç±»
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


# ======================
# ç»˜å›¾å‡½æ•°
# ======================
def plot_metrics(history, save_path):
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Plot Loss
    axes[0, 0].plot(history['train_loss'], label='Training Loss')
    axes[0, 0].plot(history['val_loss'], label='Validation Loss')
    axes[0, 0].set_title('Model Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # Plot Accuracy
    axes[0, 1].plot(history['train_acc'], label='Training Acc')
    axes[0, 1].plot(history['val_acc'], label='Validation Acc')
    axes[0, 1].set_title('Model Accuracy')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # Plot F1-Score
    axes[1, 0].plot(history['train_f1'], label='Training F1')
    axes[1, 0].plot(history['val_f1'], label='Validation F1')
    axes[1, 0].set_title('Model F1-Score')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('F1-Score')
    axes[1, 0].legend()
    axes[1, 0].grid(True)

    # Plot AUC
    axes[1, 1].plot(history['train_auc'], label='Training AUC')
    axes[1, 1].plot(history['val_auc'], label='Validation AUC')
    axes[1, 1].set_title('Model AUC')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('AUC')
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()


def plot_confusion_matrix(cm, classes, save_path):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.savefig(save_path)
    plt.close()


# ======================
# ä¸»è®­ç»ƒæµç¨‹
# ======================
def main():
    print("ğŸš€ å¼€å§‹è®­ç»ƒè„‘å‡ºè¡€æ£€æµ‹æ¨¡å‹...")
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    print(f"âœ… éšæœºç§å­: {SEED}")

    # 1. åŠ è½½æ ‡ç­¾
    print("âœ… åŠ è½½æ ‡ç­¾æ–‡ä»¶...")
    labels_df = pd.read_csv(LABELS_FILE)
    labels_df.columns = labels_df.columns.str.strip()  # æ¸…ç†åˆ—å

    if 'hemorrhage' not in labels_df.columns:
        raise ValueError(f"CSV å¿…é¡»åŒ…å« 'hemorrhage' åˆ—ï¼å½“å‰åˆ—: {list(labels_df.columns)}")

    print("\nğŸ“Š åŸå§‹æ ‡ç­¾åˆ†å¸ƒ:")
    label_counts = labels_df['hemorrhage'].value_counts()
    print(label_counts)
    print(f"æ€»æ ·æœ¬æ•°: {len(labels_df)}")

    # --- å…³é”®æ­¥éª¤ï¼šæ‰“ä¹±æ•°æ®æ¡† ---
    print("\nğŸ”„ æ‰“ä¹±æ•°æ®é¡ºåº...")
    labels_df_shuffled = labels_df.sample(frac=1, random_state=SEED).reset_index(drop=True)
    print("ğŸ“Š æ‰“ä¹±åæ ‡ç­¾åˆ†å¸ƒ:")
    print(labels_df_shuffled['hemorrhage'].value_counts())

    # 2. å‡†å¤‡æ•°æ®
    image_ids = labels_df_shuffled['id'].tolist()
    labels = labels_df_shuffled['hemorrhage'].tolist()

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† (æŒ‰æ ‡ç­¾æ¯”ä¾‹åˆ†å±‚æŠ½æ ·) - ä»æ‰“ä¹±åçš„æ•°æ®ä¸­åˆ’åˆ†
    train_ids, val_ids, train_labels, val_labels = train_test_split(
        image_ids, labels, test_size=0.2, random_state=SEED, stratify=labels
    )

    print(f"\nğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_ids)}, éªŒè¯é›†å¤§å°: {len(val_ids)}")
    print(f"ğŸ“Š è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {Counter(train_labels)}")
    print(f"ğŸ“Š éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {Counter(val_labels)}")

    # æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒé›†ï¼‰
    train_transform = transforms.Compose([
        transforms.Resize(IMAGE_SIZE),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),  # å¢åŠ å‚ç›´ç¿»è½¬
        transforms.RandomRotation(degrees=15),  # å¢åŠ æ—‹è½¬è§’åº¦
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05, hue=0.05),  # å¢åŠ é¢œè‰²æŠ–åŠ¨
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    val_transform = transforms.Compose([
        transforms.Resize(IMAGE_SIZE),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = HemorrhageDataset(train_ids, train_labels, transform=train_transform)
    val_dataset = HemorrhageDataset(val_ids, val_labels, transform=val_transform)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

    # 3. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = Classifier().to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    # ä½¿ç”¨ AdamW å’Œ weight_decayï¼Œå­¦ä¹ ç‡ç¨ä½
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    # ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼Œå¯èƒ½æ¯” ReduceLROnPlateau æ›´å¹³æ»‘
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-7)

    print(f"âœ… æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\n")

    # 4. è®­ç»ƒå¾ªç¯
    best_val_f1 = 0.0
    best_val_auc = 0.0
    patience_counter = 0
    history = {
        'train_loss': [], 'val_loss': [],
        'train_acc': [], 'val_acc': [],
        'train_f1': [], 'val_f1': [],
        'train_auc': [], 'val_auc': []
    }

    for epoch in range(EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        correct_train = 0
        total_train = 0
        all_train_preds = []
        all_train_targets = []
        all_train_probs = []

        for images, targets in train_loader:
            images, targets = images.to(DEVICE), targets.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

            _, predicted = outputs.max(1)
            total_train += targets.size(0)
            correct_train += predicted.eq(targets).sum().item()

            all_train_preds.extend(predicted.cpu().numpy())
            all_train_targets.extend(targets.cpu().numpy())
            # ä¿®å¤é”™è¯¯ï¼šä½¿ç”¨ detach() åˆ†ç¦»åå†è½¬ numpy
            all_train_probs.extend(torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy())

        # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
        avg_train_loss = train_loss / len(train_loader)
        train_acc = correct_train / total_train
        train_f1 = \
        classification_report(all_train_targets, all_train_preds, target_names=['No Hemorrhage', 'Hemorrhage'],
                              zero_division=0, output_dict=True)['weighted avg']['f1-score']
        train_auc = roc_auc_score(all_train_targets, all_train_probs) if len(set(all_train_targets)) > 1 else float(
            'nan')

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        all_val_preds = []
        all_val_targets = []
        all_val_probs = []

        with torch.no_grad():
            for images, targets in val_loader:
                images, targets = images.to(DEVICE), targets.to(DEVICE)
                outputs = model(images)
                loss = criterion(outputs, targets)
                val_loss += loss.item()

                # ä¿®å¤é”™è¯¯ï¼šä½¿ç”¨ detach() åˆ†ç¦»åå†è½¬ numpy
                probs = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()
                all_val_probs.extend(probs)

                _, predicted = outputs.max(1)
                all_val_preds.extend(predicted.cpu().numpy())
                all_val_targets.extend(targets.cpu().numpy())

        # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
        avg_val_loss = val_loss / len(val_loader)

        correct_val = sum(p == t for p, t in zip(all_val_preds, all_val_targets))
        val_acc = correct_val / len(all_val_targets)

        val_f1 = classification_report(all_val_targets, all_val_preds, target_names=['No Hemorrhage', 'Hemorrhage'],
                                       zero_division=0, output_dict=True)['weighted avg']['f1-score']
        val_auc = roc_auc_score(all_val_targets, all_val_probs) if len(set(all_val_targets)) > 1 else float('nan')

        # æ›´æ–°å†å²è®°å½•
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1)
        history['train_auc'].append(train_auc)
        history['val_auc'].append(val_auc)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # æ—©åœé€»è¾‘ - åŸºäº F1 åˆ†æ•°
        if val_f1 > best_val_f1 or (abs(val_f1 - best_val_f1) < 1e-4 and val_auc > best_val_auc):
            best_val_f1 = val_f1
            best_val_auc = val_auc
            patience_counter = 0

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'val_loss': avg_val_loss,
                'val_f1': val_f1,
                'val_auc': val_auc,
                'history': history
            }, MODEL_SAVE_PATH)
            print(f"ğŸ† Epoch {epoch + 1}: ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {val_f1:.4f}, AUC: {val_auc:.4f})")
        else:
            patience_counter += 1

        # æ‰“å°æ—¥å¿—
        if epoch % 10 == 0 or epoch == EPOCHS - 1 or patience_counter == 0:
            print(f"Epoch [{epoch + 1}/{EPOCHS}] | "
                  f"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | "
                  f"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | "
                  f"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | "
                  f"Train AUC: {train_auc:.4f} | Val AUC: {val_auc:.4f}")

        # æ£€æŸ¥æ—©åœ
        if patience_counter >= PATIENCE:
            print(f"Early stopping triggered after {PATIENCE} epochs without improvement in F1/AUC.")
            break

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ† æœ€ä½³éªŒè¯ F1 åˆ†æ•°: {best_val_f1:.4f}")
    print(f"ğŸ† æœ€ä½³éªŒè¯ AUC: {best_val_auc:.4f}")
    print(f"ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {MODEL_SAVE_PATH}")

    # ç»˜åˆ¶å¹¶ä¿å­˜ç»“æœå›¾è¡¨
    plot_metrics(history, "results/training_history.png")
    print("ğŸ“Š è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜è‡³ results/training_history.png")

    # åœ¨æœ€ç»ˆéªŒè¯é›†ä¸Šç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    final_cm = confusion_matrix(all_val_targets, all_val_preds)
    plot_confusion_matrix(final_cm, ['No Hemorrhage', 'Hemorrhage'], "results/confusion_matrix_final.png")
    print("ğŸ“Š æœ€ç»ˆæ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³ results/confusion_matrix_final.png")

    final_cr = classification_report(all_val_targets, all_val_preds, target_names=['No Hemorrhage', 'Hemorrhage'],
                                     output_dict=True)
    print("\nğŸ“‹ æœ€ç»ˆéªŒè¯é›†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_val_targets, all_val_preds, target_names=['No Hemorrhage', 'Hemorrhage']))


if __name__ == "__main__":
    main()